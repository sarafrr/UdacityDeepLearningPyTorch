{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "b3e11302396c016a9339d983aa572f04903a52d62a8b4b88f237b03ab4fe872b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Tensors in PyTorch\n",
    "First of all, import the library [PyTorch](https://pytorch.org/). Such library works really similar to **NumPy** one which is really good in processing arrays.\n",
    "The main feature of **PyTorch** is the type **tensor** which is very similar to **NumPy** arrays with the capacity to be processed very quickly by GPUs.\n",
    "Moreover, it also provide a module for computing the gradient (very useful for backpropagation in neural networks!)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    \"\"\" Sigmoid activation function\n",
    "\n",
    "        Arguments\n",
    "        -----------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "source": [
    "We constuct a list of one vector of 1 row and 5 columns. Features are in general in the columns.\n",
    "We create a vector of numbers which are taken by a *normal distribution* (gaussian with mean of zero and standard deviation of one).\n",
    "For the weights we do the same (we have to have a weight for each feature). Moreover, we create the bias term sampled by the same distribution."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-0.1468,  0.7861,  0.9468, -1.1143,  1.6908]])\ntorch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(7)\n",
    "features = torch.randn((1,5))\n",
    "print(features)\n",
    "print(features.type())\n",
    "\n",
    "weights = torch.randn_like(features)\n",
    "bias = torch.randn((1,1))"
   ]
  },
  {
   "source": [
    "We compute the output of the network to the single sample.\n",
    "In the case of **PyTorch**, we have the inner product/dot product for doing the matrix multiplication: thanks to `torch.mm()` and `toch.matmult()`.\n",
    "The second fuction is more complicate and support broadcasting.\n",
    "In our case, we have the tensors `features` and `weights` of the same shape, so if we want multiply them we have to transpose the second.\n",
    "In the end, we want to have as *output a **tensor** with the **number of rows equal to the number of samples in input***.\n",
    "\n",
    "## Reshape tensors\n",
    "1. [`weights.reshape(a,b)`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape) sometimes returns a new tensor of given size, and sometimes returns a clone with the given size (so it uses a new part of memory).\n",
    "2. [`weights.resize_(a,b)`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.resize_) returns the same tensor with a different shape. However, if the new shape results in fewer elements than the original tensor, some elements will be removed from the tensor (but not from memory). If the new shape results in more elements than the original tensor, new elements will be uninitialized in memory. The underscore at the end of the method denotes that this method is performed in-place.\n",
    "3. [`weights.view(a,b)`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view) will return a new tensor with the data in `weights` and the size `(a,b)`\n",
    "\n",
    "As rule of thumb: use `tensor.view(a,b)` in order to change the dimension of a tensor."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The output of the single layer network is:  tensor([[0.1595]])\nThe output of the single layer network is:  tensor([[0.1595]])\n"
     ]
    }
   ],
   "source": [
    "output = torch.sum(features*weights)+bias\n",
    "output = activation(output)\n",
    "print('The output of the single layer network is: ', output)\n",
    "\n",
    "weights = weights.view(5,1)\n",
    "output1 = torch.mm(features, weights)\n",
    "output1 = output1+bias\n",
    "output1 = activation(output1)\n",
    "print('The output of the single layer network is: ', output1)"
   ]
  },
  {
   "source": [
    "Now, we want to consider a 2 layer network."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate some data\n",
    "torch.manual_seed(7) # Set the random seed so things are predictable\n",
    "\n",
    "# Features are 3 random normal variables\n",
    "features = torch.randn((1, 3))\n",
    "\n",
    "# Define the size of each layer in our network\n",
    "n_input = features.shape[1]     # Number of input units, must match number of input features\n",
    "n_hidden = 2                    # Number of hidden units \n",
    "n_output = 1                    # Number of output units\n",
    "\n",
    "# Weights for inputs to hidden layer\n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "# Weights for hidden layer to output layer\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "# and bias terms for hidden and output layers\n",
    "B1 = torch.randn((1, n_hidden))\n",
    "B2 = torch.randn((1, n_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The output of the double layer network is:  tensor([[0.3171]])\n"
     ]
    }
   ],
   "source": [
    "h1 = activation(torch.mm(features,W1)+B1)\n",
    "output2 = activation(torch.mm(h1,W2)+B2)\n",
    "\n",
    "print('The output of the double layer network is: ', output2)"
   ]
  },
  {
   "source": [
    "All the other parameters (than the weights and the biases) which the programmer has to choose are named **hyperparameters**.\n",
    "Generally speaking, it is known that the more the number of hidden units and the more the number of hidden layers a network has, the more such network is accurate (**on the training set**)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Numpy to torch and back\n",
    "PyTorch has a very useful tool to make such conversions (from PyTorch to Numpy and viceversa):\n",
    "- `torch.from_numpy()` to convert a numpy tensor to a torch tensor\n",
    "- `numpy()` to get the numpy version of a torch tensor\n",
    "\n",
    "Note: In the end we have one single memory area. Indeed, the *memory is shared between the numpy array and the torch tensor*, so, *modifying in place the element of one or the other modifies also the other*.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.6166, 0.7356, 0.8789]], dtype=torch.float64)\nType of b:  torch.DoubleTensor\nType of c:  <class 'numpy.ndarray'>\n[[1.23327644 1.47124599 1.75777034]]\n[[1.23327644 1.47124599 1.75777034]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.rand(1,3)\n",
    "b = torch.from_numpy(a)\n",
    "print(b)\n",
    "print(\"Type of b: \", b.type())\n",
    "c = b.numpy()\n",
    "print(\"Type of c: \", type(c))\n",
    "b.mul_(2)\n",
    "print(a)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}