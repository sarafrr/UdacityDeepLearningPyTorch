{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "03877239ae18a66c3d863e76fda72b4917823ea0b4359fa63eeccf9c06b3cab7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Saving and Loading Models\n",
    "We will see how to save and load models in PyTorch."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import helper\n",
    "import fc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to .\\F_MNIST\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n",
      "100.0%\n",
      "Extracting .\\F_MNIST\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to .\\F_MNIST\\FashionMNIST\\raw\n",
      "13.9%\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to .\\F_MNIST\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "100.6%\n",
      "0.3%Extracting .\\F_MNIST\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to .\\F_MNIST\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to .\\F_MNIST\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "100.0%\n",
      "Extracting .\\F_MNIST\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to .\\F_MNIST\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to .\\F_MNIST\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "119.3%Extracting .\\F_MNIST\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to .\\F_MNIST\\FashionMNIST\\raw\n",
      "\n",
      "Processing...\n",
      "Done!\n",
      "\n",
      "C:\\Users\\sferro\\ProjectsCode\\OtherStuff\\PyScripts\\UdacityDeepLearningPyTorch\\venv\\lib\\site-packages\\torchvision\\datasets\\mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('.\\\\F_MNIST\\\\', download=False, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('.\\\\F_MNIST\\\\', download=False, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "image/png": {
       "width": 231,
       "height": 231
      },
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "image, label = next(iter(trainloader))\n",
    "# we take the first image of a batch of the train set\n",
    "helper.imshow(image[0,:]);"
   ]
  },
  {
   "source": [
    "## Train a Network\n",
    "The network defined in a previous notebook is reported in the module `fc_model`. Importing this, we can easily create a fully connected layer with `fc_model.Network` and train it with `fc_model.train`. We are going to save the model after it is trained. And then we will see how to load the model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1/2..  Training Loss: 1.497..  Test Loss: 0.886..  Test Accuracy: 0.638\n",
      "Epoch: 1/2..  Training Loss: 0.994..  Test Loss: 0.757..  Test Accuracy: 0.692\n",
      "Epoch: 1/2..  Training Loss: 0.885..  Test Loss: 0.680..  Test Accuracy: 0.747\n",
      "Epoch: 1/2..  Training Loss: 0.812..  Test Loss: 0.641..  Test Accuracy: 0.752\n",
      "Epoch: 1/2..  Training Loss: 0.806..  Test Loss: 0.637..  Test Accuracy: 0.758\n",
      "Epoch: 1/2..  Training Loss: 0.793..  Test Loss: 0.664..  Test Accuracy: 0.750\n",
      "Epoch: 1/2..  Training Loss: 0.789..  Test Loss: 0.653..  Test Accuracy: 0.764\n",
      "Epoch: 1/2..  Training Loss: 0.795..  Test Loss: 0.633..  Test Accuracy: 0.770\n",
      "Epoch: 1/2..  Training Loss: 0.779..  Test Loss: 0.613..  Test Accuracy: 0.771\n",
      "Epoch: 1/2..  Training Loss: 0.780..  Test Loss: 0.618..  Test Accuracy: 0.759\n",
      "Epoch: 1/2..  Training Loss: 0.760..  Test Loss: 0.614..  Test Accuracy: 0.769\n",
      "Epoch: 1/2..  Training Loss: 0.752..  Test Loss: 0.617..  Test Accuracy: 0.777\n",
      "Epoch: 1/2..  Training Loss: 0.774..  Test Loss: 0.578..  Test Accuracy: 0.785\n",
      "Epoch: 1/2..  Training Loss: 0.746..  Test Loss: 0.580..  Test Accuracy: 0.784\n",
      "Epoch: 1/2..  Training Loss: 0.717..  Test Loss: 0.567..  Test Accuracy: 0.791\n",
      "Epoch: 1/2..  Training Loss: 0.751..  Test Loss: 0.565..  Test Accuracy: 0.799\n",
      "Epoch: 1/2..  Training Loss: 0.699..  Test Loss: 0.560..  Test Accuracy: 0.800\n",
      "Epoch: 1/2..  Training Loss: 0.763..  Test Loss: 0.551..  Test Accuracy: 0.802\n",
      "Epoch: 1/2..  Training Loss: 0.738..  Test Loss: 0.581..  Test Accuracy: 0.788\n",
      "Epoch: 1/2..  Training Loss: 0.721..  Test Loss: 0.562..  Test Accuracy: 0.793\n",
      "Epoch: 1/2..  Training Loss: 0.771..  Test Loss: 0.548..  Test Accuracy: 0.797\n",
      "Epoch: 1/2..  Training Loss: 0.712..  Test Loss: 0.548..  Test Accuracy: 0.797\n",
      "Epoch: 1/2..  Training Loss: 0.722..  Test Loss: 0.571..  Test Accuracy: 0.805\n",
      "Epoch: 2/2..  Training Loss: 0.715..  Test Loss: 0.549..  Test Accuracy: 0.811\n",
      "Epoch: 2/2..  Training Loss: 0.726..  Test Loss: 0.553..  Test Accuracy: 0.805\n",
      "Epoch: 2/2..  Training Loss: 0.698..  Test Loss: 0.556..  Test Accuracy: 0.801\n",
      "Epoch: 2/2..  Training Loss: 0.695..  Test Loss: 0.567..  Test Accuracy: 0.807\n",
      "Epoch: 2/2..  Training Loss: 0.738..  Test Loss: 0.540..  Test Accuracy: 0.805\n",
      "Epoch: 2/2..  Training Loss: 0.736..  Test Loss: 0.568..  Test Accuracy: 0.809\n",
      "Epoch: 2/2..  Training Loss: 0.738..  Test Loss: 0.555..  Test Accuracy: 0.794\n",
      "Epoch: 2/2..  Training Loss: 0.708..  Test Loss: 0.544..  Test Accuracy: 0.805\n",
      "Epoch: 2/2..  Training Loss: 0.651..  Test Loss: 0.546..  Test Accuracy: 0.810\n",
      "Epoch: 2/2..  Training Loss: 0.699..  Test Loss: 0.555..  Test Accuracy: 0.809\n",
      "Epoch: 2/2..  Training Loss: 0.724..  Test Loss: 0.538..  Test Accuracy: 0.822\n",
      "Epoch: 2/2..  Training Loss: 0.694..  Test Loss: 0.551..  Test Accuracy: 0.809\n",
      "Epoch: 2/2..  Training Loss: 0.705..  Test Loss: 0.543..  Test Accuracy: 0.802\n",
      "Epoch: 2/2..  Training Loss: 0.629..  Test Loss: 0.533..  Test Accuracy: 0.812\n",
      "Epoch: 2/2..  Training Loss: 0.728..  Test Loss: 0.521..  Test Accuracy: 0.822\n",
      "Epoch: 2/2..  Training Loss: 0.729..  Test Loss: 0.554..  Test Accuracy: 0.815\n",
      "Epoch: 2/2..  Training Loss: 0.698..  Test Loss: 0.522..  Test Accuracy: 0.824\n",
      "Epoch: 2/2..  Training Loss: 0.678..  Test Loss: 0.531..  Test Accuracy: 0.811\n",
      "Epoch: 2/2..  Training Loss: 0.710..  Test Loss: 0.539..  Test Accuracy: 0.814\n",
      "Epoch: 2/2..  Training Loss: 0.657..  Test Loss: 0.540..  Test Accuracy: 0.810\n",
      "Epoch: 2/2..  Training Loss: 0.695..  Test Loss: 0.525..  Test Accuracy: 0.819\n",
      "Epoch: 2/2..  Training Loss: 0.703..  Test Loss: 0.515..  Test Accuracy: 0.822\n",
      "Epoch: 2/2..  Training Loss: 0.690..  Test Loss: 0.526..  Test Accuracy: 0.814\n"
     ]
    }
   ],
   "source": [
    "model = fc_model.Network(784, 10, [512, 256, 128])\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "fc_model.train(model, trainloader, testloader, criterion, optimizer, epochs=2)"
   ]
  },
  {
   "source": [
    "## Saving and Loading Networks\n",
    "It is impractical to train a network every time we want to use it. Insted, we can save trained network and use them when we need.  \n",
    "So, we can use them later for predictions or for continuing to train.  \n",
    "The parameters for PyTorch networks are stored in a model's `state_dict`.  \n",
    "We can see that the `state_dict` contains the weights and biases matrices for each layer of our network."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Our model: \n\n Network(\n  (hidden_layers): ModuleList(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): Linear(in_features=512, out_features=256, bias=True)\n    (2): Linear(in_features=256, out_features=128, bias=True)\n  )\n  (output): Linear(in_features=128, out_features=10, bias=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n) \n\nThe state dict keys: \n\n odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Our model: \\n\\n\", model, '\\n')\n",
    "print(\"The state dict keys: \\n\\n\", model.state_dict().keys())"
   ]
  },
  {
   "source": [
    "The simplest thing to do is simply save the state dict with `torch.save`. For example, we can save it in an object named `checkpoint.pth`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "source": [
    "Then, we can load the `state_dict()` with `torch.load()`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('checkpoint.pth')\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "source": [
    "To load the state_dict to your network we do `model.load_state_dict(state_dict)`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "source": [
    "Note: the loading of the previously saved model works only if we have the same architecture in the model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.0.weight: copying a param with shape torch.Size([512, 784]) from checkpoint, the shape in current model is torch.Size([400, 784]).\n\tsize mismatch for hidden_layers.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([400]).\n\tsize mismatch for hidden_layers.1.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([200, 400]).\n\tsize mismatch for hidden_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for hidden_layers.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([100, 200]).\n\tsize mismatch for hidden_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for output.weight: copying a param with shape torch.Size([10, 128]) from checkpoint, the shape in current model is torch.Size([10, 100]).",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-d859c59ebec0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfc_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# This will throw an error because the tensor sizes are wrong!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\ProjectsCode\\OtherStuff\\PyScripts\\UdacityDeepLearningPyTorch\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1223\u001b[1;33m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[0;32m   1224\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0;32m   1225\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.0.weight: copying a param with shape torch.Size([512, 784]) from checkpoint, the shape in current model is torch.Size([400, 784]).\n\tsize mismatch for hidden_layers.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([400]).\n\tsize mismatch for hidden_layers.1.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([200, 400]).\n\tsize mismatch for hidden_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for hidden_layers.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([100, 200]).\n\tsize mismatch for hidden_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for output.weight: copying a param with shape torch.Size([10, 128]) from checkpoint, the shape in current model is torch.Size([10, 100])."
     ]
    }
   ],
   "source": [
    "# Try this\n",
    "model = fc_model.Network(784, 10, [400, 200, 100])\n",
    "# This will throw an error because the tensor sizes are wrong!\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "source": [
    "This means that we have to build the model exactly as it was when trained. Information about the model architecture has to be saved in the checkpoint, along with the state dict.  \n",
    "To do this, we have to build a dictionary with all the information about the architecture of the network we are going to save."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'input_size': 784,\n",
    "              'output_size': 10,\n",
    "              'hidden_layers': [each.out_features for each in model.hidden_layers],\n",
    "              'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, 'checkpoint.pth')"
   ]
  },
  {
   "source": [
    "Now, the checkpoint model has all the information to rebuild the train model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = fc_model.Network(checkpoint['input_size'],\n",
    "                             checkpoint['output_size'],\n",
    "                             checkpoint['hidden_layers'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Network(\n  (hidden_layers): ModuleList(\n    (0): Linear(in_features=784, out_features=400, bias=True)\n    (1): Linear(in_features=400, out_features=200, bias=True)\n    (2): Linear(in_features=200, out_features=100, bias=True)\n  )\n  (output): Linear(in_features=100, out_features=10, bias=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n)\n"
     ]
    }
   ],
   "source": [
    "model = load_checkpoint('checkpoint.pth')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}